{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load module\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.lm import Vocabulary\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read processed file\n",
    "filename = 'D://Users/figohjs/Documents/NLP/StrPrioritization/Data/Interim/2020-04-24_ProcessedDF.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "#define column names\n",
    "truePositiveCol = 'TP'\n",
    "textCol = 'SUSPICION_DESC_CLEAN'\n",
    "recordIdCol = 'RECORD_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bad index:index for bad report\n",
    "badIndex = list(df[df['TP']].index)\n",
    "\n",
    "#build vocabulary to determine vector size\n",
    "tokenizedList = [re.sub('\\s+',' ',str(i)).split(' ') for i in df[textCol].values]\n",
    "flatTokenizedList = list(itertools.chain(*tokenizedList))\n",
    "vocab = Vocabulary(flatTokenizedList)\n",
    "\n",
    "#create dictionaries\n",
    "IndexToReportID_Dict = df[recordIdCol].to_dict()\n",
    "IndexToStrDesc_Dict = df[textCol].to_dict()\n",
    "ReportIDToIndex_Dict = {j:i for i,j in IndexToReportID_Dict.items()}\n",
    "\n",
    "#docs to tag\n",
    "docs = df[textCol].values\n",
    "\n",
    "#docs to tag\n",
    "#fillna with ''\n",
    "df[textCol].fillna('', inplace = True)\n",
    "docs = df[textCol].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load module\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class docEmbedding():\n",
    "    def __init__(self, **kwargs):\n",
    "        #predefine prameters\n",
    "        self.Doc2VecModel = Doc2Vec(**kwargs)\n",
    "        #where to save model\n",
    "        self.ModelFolder = 'D:/Users/figohjs/Documents/NLP/StrPrioritization/Streamlit/Model'\n",
    "        #model name\n",
    "        self.ModelName = self.ModelFolder + '/' + datetime.now().strftime('%Y-%m-%d') + '_' + 'Doc2Vec.model'\n",
    "        #indices are based on list of tagged document\n",
    "        self.BadStrIndex = kwargs['bad_index']\n",
    "        self.Threshold = kwargs['threshold']\n",
    "        self.VectorSize = kwargs['vector_size']\n",
    "        self.DM = kwargs['dm']\n",
    "        self.MaxEpoch = kwargs['epoch']\n",
    "\n",
    "    def tagDocument(self, ContentList):\n",
    "        #tag document\n",
    "        self.TaggedDocList = [TaggedDocument(doc[1], tags = [doc[0]]) \n",
    "                              for doc in zip(self.BadStrIndex, [i.split() for i in ContentList])]\n",
    "    \n",
    "    def trainModel(self):\n",
    "        #build vocab\n",
    "        self.Doc2VecModel.build_vocab(self.TaggedDocList)\n",
    "        \n",
    "        for epoch in range(self.MaxEpoch):\n",
    "            print('Iteration - %s'%epoch+1)\n",
    "            #total_examples = total of document\n",
    "            self.Doc2VecModel.train(self.TaggedDocList,\n",
    "                                    total_examples = self.Doc2VecModel.corpus_count,\n",
    "                                    epochs = epoch)\n",
    "        #self.Doc2VecModel.alpha -= 0.002\n",
    "        self.Doc2VecModel.save(self.ModelName)\n",
    "        print('%s is saved'%self.ModelName)\n",
    "        \n",
    "    def loadModel(self, TrainedModel):\n",
    "        self.Doc2VecModel = gensim.models.doc2vec.Doc2Vec.load(TrainedModel)\n",
    "        \n",
    "    def findSimilarDocForBadDoc(self, NewDoc, n = 10):\n",
    "        NewVec = self.Doc2VecModel.infer_vector(NewDoc.split())\n",
    "        similarRepList = []\n",
    "        similarScoreList = []\n",
    "\n",
    "        #loop through every bad report\n",
    "        for index in self.BadStrIndex:\n",
    "            similarScore = cosine_similarity(self.Doc2VecModel.docvecs[index].reshape(1, self.VectorSize),\n",
    "                                             NewVec.reshape(1, self.VectorSize))\n",
    "            #if similarity score exceeds threshold\n",
    "            if similarScore >= self.Threshold:\n",
    "                similarRepList.append(str(index))\n",
    "                similarScoreList.append(str(similarScore[0][0]))\n",
    "\n",
    "        #if not similar to any bad report, (a, b, c)\n",
    "\n",
    "        #a - 1 if has similar to any bad report\n",
    "        #b - list of reports \n",
    "        if len(similarRepList) == 0:\n",
    "            return (0, '', '')\n",
    "        else:\n",
    "            return (1, ','.join(similarRepList), ','.join(similarScoreList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters for neural network\n",
    "#use distributed memory in default: predict word given context\n",
    "dm = 1\n",
    "epoch = 10\n",
    "#use google recommended vector size\n",
    "vectorSize = int(round((len(vocab) - 1)**(1/4), 0))\n",
    "#learning rate\n",
    "alpha = 0.025\n",
    "minAlpha = alpha #no decreasing rate\n",
    "#minimum frequency of token\n",
    "minCount = 1\n",
    "#true positive as bad reports\n",
    "badIndex = list(df[df['TP']].index)\n",
    "#threshold of similarity score to be considered as similar\n",
    "threshold = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 0\n",
      "Iteration - 1\n",
      "Iteration - 2\n",
      "Iteration - 3\n",
      "Iteration - 4\n",
      "Iteration - 5\n",
      "Iteration - 6\n",
      "Iteration - 7\n",
      "Iteration - 8\n",
      "Iteration - 9\n",
      "D:/Users/figohjs/Documents/NLP/StrPrioritization/Streamlit/Model/2020-04-27_Doc2Vec.model is saved\n"
     ]
    }
   ],
   "source": [
    "#instantiate a class named similarDoc\n",
    "paramDict = {'min_count': minCount, 'vector_size': vectorSize,\n",
    "             'alpha':alpha, 'min_alpha':alpha, 'bad_index':badIndex,\n",
    "             'threshold':threshold, 'dm':dm, 'epoch':epoch}\n",
    "\n",
    "nlpSimilarity = docEmbedding(**paramDict)\n",
    "\n",
    "#tag documents\n",
    "nlpSimilarity.tagDocument(docs)\n",
    "\n",
    "#train model\n",
    "nlpSimilarity.trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load trained model\n",
    "trainedModel = 'D:/Users/figohjs/Documents/NLP/StrPrioritization/Streamlit/Model/2020-04-27_Doc2Vec.model'\n",
    "nlpSimilarity.loadModel(trainedModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "#use trained model to find if there is report similar with any bad reports\n",
    "similarList = []\n",
    "indexList = []\n",
    "scoreList = []\n",
    "\n",
    "dfSubset = df[~df['TP']].copy()\n",
    "\n",
    "#use row which is not true positive i.e not bad report\n",
    "for no in range(dfSubset.shape[0]):\n",
    "    info = dfSubset['SUSPICION_DESC_CLEAN'].values[no]\n",
    "    similarBool, index, score = nlpSimilarity.findSimilarDocForBadDoc(info)\n",
    "    similarList.append(similarBool)\n",
    "    indexList.append(index)\n",
    "    scoreList.append(score)\n",
    "pandaDict = {'similarBool':similarList,\n",
    "            'reportIndexList':indexList,\n",
    "            'scoreList':scoreList}\n",
    "dfResult = pd.DataFrame(pandaDict)\n",
    "dfResult = pd.concat([dfResult, df.reset_index()], axis = 1)\n",
    "dfResult.fillna('', inplace = True)\n",
    "\n",
    "end = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in minutes: 23\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken in minutes: %s\" %round((start - end).seconds//3600, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6418, 29)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfResult.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TestEnv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
