{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "#Load module\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training file\n",
    "filename = 'D://Users/figohjs/Documents/NLP/StrPrioritization/Data/Interim/2020-03-22_TrainingData.csv'\n",
    "dfTraining = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read testing file\n",
    "filename = 'D://Users/figohjs/Documents/NLP/StrPrioritization/Data/Interim/2020-03-22_TestingData.csv'\n",
    "dfTesting = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save smaller testing file\n",
    "filename = 'D://Users/figohjs/Documents/NLP/StrPrioritization/Data/Interim/2020-03-24_SmallTestingData.csv'\n",
    "dfTesting.iloc[:100].to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read final processed file\n",
    "filename = 'D://Users/figohjs/Documents/NLP/StrPrioritization/Data/Interim/2020-03-24_FinalResult.csv'\n",
    "dfFinal = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfFinal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionaries\n",
    "IndexToReportID_Dict = dfTraining['RECORD_ID'].to_dict()\n",
    "IndexToStrDesc_Dict = dfTraining['SUSPICION_DESC'].to_dict()\n",
    "ReportIDToIndex_Dict = {j:i for i,j in IndexToReportID_Dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters for neural network\n",
    "maxEpoch = 5\n",
    "vectorSize = 500\n",
    "alpha = 0.025\n",
    "minAlpha = alpha #no decreasing rate\n",
    "minCount = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load module\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "#doc2Vec model\n",
    "class similarDoc():\n",
    "    def __init__(self, **kwargs):\n",
    "        #predefine prameters\n",
    "        self.Doc2VecModel = Doc2Vec(**kwargs)\n",
    "        self.MaxEpoch = 10\n",
    "        self.ModelFolder = 'D:/Users/figohjs/Documents/NLP/StrPrioritization/Model'\n",
    "        self.ModelName = self.ModelFolder + '/' + datetime.now().strftime('%Y-%m-%d') + '_' + 'Doc2Vec.model'\n",
    "        #indices are based on list of tagged document\n",
    "        self.BadStrIndex = kwargs['bad_index']\n",
    "        self.Threshold = kwargs['threshold']\n",
    "        self.VectorSize = kwargs['vector_size']\n",
    "        self.SimilarBadDocList = []\n",
    "        \n",
    "    def prepareText(self, ContentList):\n",
    "        #tag document\n",
    "#         self.TaggedDocList = [TaggedDocument(doc, tags = [str(no)]) \n",
    "#                               for no, doc in enumerate([i.split() for i in ContentList])]\n",
    "        self.TaggedDocList = [TaggedDocument(doc[1], tags = [doc[0]]) \n",
    "                              for doc in zip(self.BadStrIndex, [i.split() for i in ContentList])]\n",
    "    def trainModel(self):\n",
    "        #build vocab\n",
    "        self.Doc2VecModel.build_vocab(self.TaggedDocList)\n",
    "        for epoch in range(self.MaxEpoch):\n",
    "            print('Iteration - %s'%epoch)\n",
    "            self.Doc2VecModel.train(self.TaggedDocList,\n",
    "                                    total_examples = self.Doc2VecModel.corpus_count,\n",
    "                                    epochs = epoch)\n",
    "            #self.Doc2VecModel.alpha -= 0.002\n",
    "        self.Doc2VecModel.save(self.ModelName)\n",
    "        print('%s is saved'%self.ModelName)\n",
    "        \n",
    "    def loadModel(self, TrainedModel):\n",
    "        self.Doc2VecModel = gensim.models.doc2vec.Doc2Vec.load(TrainedModel)\n",
    "                    \n",
    "    def findSimilarDocForBadDoc(self, NewDoc, n = 10):\n",
    "        NewVec = self.Doc2VecModel.infer_vector(NewDoc.split())\n",
    "        similarRepList = []\n",
    "        similarScoreList = []\n",
    "        #loop through every bad report\n",
    "        for index in self.BadStrIndex:\n",
    "            similarScore = cosine_similarity(self.Doc2VecModel.docvecs[index].reshape(1, self.VectorSize),\n",
    "                                             NewVec.reshape(1, self.VectorSize))\n",
    "            #if similarity score exceeds threshold\n",
    "            if similarScore >= self.Threshold:\n",
    "                similarRepList.append(str(index))\n",
    "                similarScoreList.append(str(similarScore[0][0]))\n",
    "        #if not similar to any bad report, (a, b, c)\n",
    "        #a - 1 if has similar to any bad report\n",
    "        #b - list of reports \n",
    "        if len(similarRepList) == 0:\n",
    "            return (0, '', '')\n",
    "        else:\n",
    "            return (1, ','.join(similarRepList), ','.join(similarScoreList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters for neural network\n",
    "maxEpoch = 5\n",
    "vectorSize = 500\n",
    "alpha = 0.025\n",
    "minAlpha = alpha #no decreasing rate\n",
    "minCount = 1\n",
    "badIndex = list(dfTraining[dfTraining['TP']].index)\n",
    "#instantiate a class named similarDoc\n",
    "paramDict = {'min_count': minCount, 'vector_size': vectorSize,\n",
    "             'alpha':alpha, 'min_alpha':alpha, 'bad_index':badIndex,\n",
    "            'threshold':0.75}\n",
    "\n",
    "nlpSimilarity = similarDoc(**paramDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.58 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "#load model\n",
    "trainedModel = '2020-03-19_Doc2Vec.model'\n",
    "nlpSimilarity.loadModel(trainedModel)\n",
    "print(f'Time taken: {round(time.time() - start, 2)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1258.3744292259216\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#use trained model to find if there is report similar with any bad reports\n",
    "similarList = []\n",
    "indexList = []\n",
    "scoreList = []\n",
    "for no in range(dfTesting.shape[0]):\n",
    "    info = str(dfTesting['SUSPICION_DESC_CLEAN'].values[no])\n",
    "    similarBool, index, score = nlpSimilarity.findSimilarDocForBadDoc(info)\n",
    "    similarList.append(similarBool)\n",
    "    indexList.append(index)\n",
    "    scoreList.append(score)\n",
    "pandaDict = {'similarBool':similarList,\n",
    "            'reportIndexList':indexList,\n",
    "            'scoreList':scoreList}\n",
    "dfResult = pd.DataFrame(pandaDict)\n",
    "dfResult = pd.concat([dfResult, dfTesting.reset_index()], axis = 1)\n",
    "dfResult.fillna('', inplace = True)\n",
    "print(f'Time: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get most similar report\n",
    "def mostSimilarReport(indexArray, scoreArray):\n",
    "    #store result\n",
    "    resultList = []\n",
    "    \n",
    "    #loop through every row\n",
    "    for no, tempArray in enumerate(scoreArray):\n",
    "        #get max score from every list splitted by tempArray\n",
    "        tempScoreList = str(tempArray).split(',')\n",
    "        tempIndexList = str(indexArray[no]).split(',')\n",
    "        maxScore = max(tempScoreList)\n",
    "        indexList = [(i, tempScoreList[no]) for no, i in enumerate(tempIndexList) if  tempScoreList[no] == maxScore]\n",
    "        \n",
    "        if len(indexList) > 1:\n",
    "            #take first element for interim\n",
    "            resultList.append([indexList[0][0], indexList[0][1]])\n",
    "        elif len(indexList) == 1:\n",
    "            resultList.append([indexList[0][0], indexList[0][1]])\n",
    "        else:\n",
    "            resultList.append(['', ''])\n",
    "          \n",
    "    return pd.DataFrame(resultList, columns = ['mostSimilarIndex', 'mostSimilarScore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfResult2 = mostSimilarReport(dfResult['reportIndexList'].values, dfResult['scoreList'].values)\n",
    "\n",
    "colList = ['RECORD_ID', 'ANALYST','SUSPICION_DESC', 'SUSPICION_DESC_CLEAN', 'TP', 'FN', 'FP', 'noFlag']\n",
    "dfFinal = pd.concat([dfResult2, dfResult[colList].reset_index()], axis = 1).copy()\n",
    "\n",
    "dfFinal['mostSimilarReportID'] = dfFinal['mostSimilarIndex'].map(lambda x:IndexToReportID_Dict[int(x)] \n",
    "                                                                 if str(x) not in ['', 'nan'] else '')\n",
    "\n",
    "dfFinal['mostSimilarStrDesc'] = dfFinal['mostSimilarIndex'].map(lambda x:IndexToStrDesc_Dict[int(x)] \n",
    "                                                                 if str(x) not in ['', 'nan'] else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal['mostSimilarIndex'].fillna('', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save final result - save time for dashboard purpose\n",
    "filename = 'D://Users/figohjs/Documents/NLP/StrPrioritization/Data/Interim/2020-03-27_FinalResult.csv'\n",
    "dfFinal.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate : 4.83\n"
     ]
    }
   ],
   "source": [
    "#success rate - flag how many FN\n",
    "successNum = sum(np.logical_and((dfFinal['mostSimilarIndex']!='').values, dfFinal['FN'].values))\n",
    "print(\"Success Rate : %s\"%round((successNum * 100/sum(dfFinal['FN'])),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlit script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting D:\\Users\\figohjs\\Documents\\NLP\\StrPrioritization\\Model\\doc2VecModel.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile D:\\Users\\figohjs\\Documents\\NLP\\StrPrioritization\\Model\\doc2VecModel.py\n",
    "#Load module\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "#doc2Vec model\n",
    "class similarDoc():\n",
    "    def __init__(self, **kwargs):\n",
    "        #predefine prameters\n",
    "        self.Doc2VecModel = Doc2Vec(**kwargs)\n",
    "        self.MaxEpoch = 10\n",
    "        self.ModelFolder = 'D:/Users/figohjs/Documents/NLP/StrPrioritization/Model'\n",
    "        self.ModelName = self.ModelFolder + '/' + datetime.now().strftime('%Y-%m-%d') + '_' + 'Doc2Vec.model'\n",
    "        #indices are based on list of tagged document\n",
    "        self.BadStrIndex = kwargs['bad_index']\n",
    "        self.Threshold = kwargs['threshold']\n",
    "        self.VectorSize = kwargs['vector_size']\n",
    "        self.SimilarBadDocList = []\n",
    "        \n",
    "    def prepareText(self, ContentList):\n",
    "        #tag document\n",
    "#         self.TaggedDocList = [TaggedDocument(doc, tags = [str(no)]) \n",
    "#                               for no, doc in enumerate([i.split() for i in ContentList])]\n",
    "        self.TaggedDocList = [TaggedDocument(doc[1], tags = [doc[0]]) \n",
    "                              for doc in zip(self.BadStrIndex, [i.split() for i in ContentList])]\n",
    "    def trainModel(self):\n",
    "        #build vocab\n",
    "        self.Doc2VecModel.build_vocab(self.TaggedDocList)\n",
    "        for epoch in range(self.MaxEpoch):\n",
    "            print('Iteration - %s'%epoch)\n",
    "            self.Doc2VecModel.train(self.TaggedDocList,\n",
    "                                    total_examples = self.Doc2VecModel.corpus_count,\n",
    "                                    epochs = epoch)\n",
    "            #self.Doc2VecModel.alpha -= 0.002\n",
    "        self.Doc2VecModel.save(self.ModelName)\n",
    "        print('%s is saved'%self.ModelName)\n",
    "        \n",
    "    def loadModel(self, TrainedModel):\n",
    "        self.Doc2VecModel = gensim.models.doc2vec.Doc2Vec.load(TrainedModel)\n",
    "                    \n",
    "    def findSimilarDocForBadDoc(self, NewDoc, n = 10):\n",
    "        NewVec = self.Doc2VecModel.infer_vector(NewDoc.split())\n",
    "        similarRepList = []\n",
    "        similarScoreList = []\n",
    "        #loop through every bad report\n",
    "        for index in self.BadStrIndex:\n",
    "            similarScore = cosine_similarity(self.Doc2VecModel.docvecs[index].reshape(1, self.VectorSize),\n",
    "                                             NewVec.reshape(1, self.VectorSize))\n",
    "            #if similarity score exceeds threshold\n",
    "            if similarScore >= self.Threshold:\n",
    "                similarRepList.append(str(index))\n",
    "                similarScoreList.append(str(similarScore[0][0]))\n",
    "        #if not similar to any bad report, (a, b, c)\n",
    "        #a - 1 if has similar to any bad report\n",
    "        #b - list of reports \n",
    "        if len(similarRepList) == 0:\n",
    "            return (0, '', '')\n",
    "        else:\n",
    "            return (1, ','.join(similarRepList), ','.join(similarScoreList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Record</th>\n",
       "      <th>Feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>234</td>\n",
       "      <td>Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>456</td>\n",
       "      <td>No Review</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Record   Feedback\n",
       "0    123           \n",
       "1    234     Review\n",
       "2    456  No Review"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recordIdList = ['123', '234', '456']\n",
    "feedbackList = ['', 'Review', 'No Review']\n",
    "dfTemp = pd.DataFrame({'Record':recordIdList, 'Feedback':feedbackList})\n",
    "dfTemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123': '', '234': 'Review', '456': 'No Review'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictTemp = dfTemp.set_index('Record').to_dict()['Feedback']\n",
    "dictTemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Review'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feedback for recordId - 234\n",
    "dictTemp['234']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record:123 and its feedback: \n",
      "Record:234 and its feedback: Review\n",
      "Record:456 and its feedback: No Review\n"
     ]
    }
   ],
   "source": [
    "#loop through dict\n",
    "for key, value in dictTemp.items():\n",
    "    print('Record:%s' %key + ' and its feedback: %s'%value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting D:\\Users\\figohjs\\Documents\\NLP\\StrPrioritization\\Streamlit\\doc2Vec.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile D:\\Users\\figohjs\\Documents\\NLP\\StrPrioritization\\Streamlit\\doc2Vec.py\n",
    "#streamlit for training and testing\n",
    "\n",
    "#import modules\n",
    "#from one folder\n",
    "import sys\n",
    "modelFilePath = 'D:/Users/figohjs/Documents/NLP/StrPrioritization/Model'\n",
    "sys.path.append(modelFilePath)\n",
    "import doc2VecModel as docModel\n",
    "\n",
    "#normal import\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import base64\n",
    "import operator\n",
    "\n",
    "#header\n",
    "st.sidebar.header('Document Similarity')\n",
    "\n",
    "#sidebars\n",
    "option = st.sidebar.selectbox(\"Phase\", ['Model Training', \"Model Testing\"])\n",
    "\n",
    "#hyperparameters for neural network\n",
    "maxEpoch = 5\n",
    "vectorSize = 500\n",
    "alpha = 0.025\n",
    "minAlpha = alpha #no decreasing rate\n",
    "minCount = 1\n",
    "\n",
    "# read training file\n",
    "filename = 'D://Users/figohjs/Documents/NLP/StrPrioritization/Data/Interim/2020-03-22_TrainingData.csv'\n",
    "dfTraining = pd.read_csv(filename)\n",
    "\n",
    "#read testing file\n",
    "filename = 'D://Users/figohjs/Documents/NLP/StrPrioritization/Data/Interim/2020-03-24_SmallTestingData.csv'\n",
    "dfTesting = pd.read_csv(filename)\n",
    "\n",
    "#define some parameters of the script\n",
    "badIndex = list(dfTraining[dfTraining['TP']].index)\n",
    "trainingData = dfTraining['SUSPICION_DESC_CLEAN'].values\n",
    "trainButton = False\n",
    "noUploadButton = False\n",
    "dfFinal = pd.DataFrame()\n",
    "trainedModel = 'D:/Users/figohjs/Documents/NLP/StrPrioritization/Model/2020-03-23_Doc2Vec.model'\n",
    "\n",
    "#create dictionaries\n",
    "IndexToReportID_Dict = dfTraining['RECORD_ID'].to_dict()\n",
    "IndexToStrDesc_Dict = dfTraining['SUSPICION_DESC'].to_dict()\n",
    "ReportIDToIndex_Dict = {j:i for i,j in IndexToReportID_Dict.items()}\n",
    "\n",
    "#UDF\n",
    "#get most similar report\n",
    "def mostSimilarReport(indexArray, scoreArray):\n",
    "    #store result\n",
    "    resultList = []\n",
    "    \n",
    "    #loop through every row\n",
    "    for no, tempArray in enumerate(scoreArray):\n",
    "        #get max score from every list splitted by tempArray\n",
    "        tempScoreList = str(tempArray).split(',')\n",
    "        tempIndexList = str(indexArray[no]).split(',')\n",
    "        maxScore = max(tempScoreList)\n",
    "        indexList = [(i, tempScoreList[no]) for no, i in enumerate(tempIndexList) if  tempScoreList[no] == maxScore]\n",
    "        \n",
    "        if len(indexList) > 1:\n",
    "            #take first element for interim\n",
    "            resultList.append([indexList[0][0], indexList[0][1]])\n",
    "        elif len(indexList) == 1:\n",
    "            resultList.append([indexList[0][0], indexList[0][1]])\n",
    "        else:\n",
    "            resultList.append(['', ''])\n",
    "          \n",
    "    return pd.DataFrame(resultList, columns = ['mostSimilarIndex', 'mostSimilarScore'])\n",
    "\n",
    "if option == 'Model Training':\n",
    "    vectorSize = st.slider('Vector Size', 100, 1000, 500)\n",
    "    alpha = st.slider('Learning Rate - Alpha', 0.025, 1., 0.025)\n",
    "    threshold = st.slider('Threshold', 0.6, 1.0, 0.75)\n",
    "    minCount = st.slider('Minimum frequency of a term', 1, 10, 1)\n",
    "    trainButton = st.button('Train Model', key = 'train')\n",
    "    \n",
    "    #if trainButton is clicked\n",
    "    if trainButton:\n",
    "        st.markdown(\"Vector Size:%s\"%vectorSize)\n",
    "        st.markdown(\"Learning Rate - Alpha:%s\"%alpha)\n",
    "        st.markdown(\"Threshold:%s\"%threshold)\n",
    "        #instantiate a class named similarDoc\n",
    "        paramDict = {'min_count': minCount, 'vector_size': vectorSize,\n",
    "                     'alpha':alpha, 'min_alpha':alpha, 'bad_index':badIndex,\n",
    "                    'threshold':threshold}\n",
    "\n",
    "        nlpSimilarity = docModel.similarDoc(**paramDict)\n",
    "\n",
    "        #training model\n",
    "        nlpSimilarity.prepareText(trainingData)\n",
    "        with st.spinner(\"Training a model\"):\n",
    "            nlpSimilarity.trainModel()\n",
    "        st.success(\"Model is trained\")\n",
    "        st.markdown(datetime.now().strftime('%Y-%m-%d') + '_' + 'Doc2Vec.model is saved')\n",
    "    \n",
    "elif option == 'Model Testing':\n",
    "    #upload file\n",
    "    file = st.file_uploader(\"Upload file\", type = ['csv'])\n",
    "    noUploadButton = st.button('No Upload', key = 'upload')\n",
    "    if noUploadButton:\n",
    "        if trainButton:\n",
    "            paramDict = {'min_count': minCount, 'vector_size': vectorSize,\n",
    "                         'alpha':alpha, 'min_alpha':alpha, 'bad_index':badIndex,\n",
    "                        'threshold':threshold}\n",
    "\n",
    "            nlpSimilarity = docModel.similarDoc(**paramDict)\n",
    "        else:\n",
    "            maxEpoch = 5\n",
    "            vectorSize = 500\n",
    "            alpha = 0.025\n",
    "            minAlpha = alpha #no decreasing rate\n",
    "            minCount = 1\n",
    "            threshold = 0.75\n",
    "            paramDict = {'min_count': minCount, 'vector_size': vectorSize,\n",
    "                 'alpha':alpha, 'min_alpha':alpha, 'bad_index':badIndex,\n",
    "                'threshold':threshold}\n",
    "\n",
    "            nlpSimilarity = docModel.similarDoc(**paramDict)\n",
    "\n",
    "        #Displaying info from model we are using\n",
    "        st.markdown(\"Vector Size:%s\"%vectorSize)\n",
    "        st.markdown(\"Learning Rate - Alpha:%s\"%alpha)\n",
    "        st.markdown(\"Threshold:%s\"%threshold)\n",
    "\n",
    "        #Loading a trained model\n",
    "        with st.spinner(\"Loading a trained model\"): \n",
    "            start = time.time()\n",
    "            nlpSimilarity.loadModel(trainedModel)\n",
    "        st.success(\"A trained model is loaded\")\n",
    "        st.markdown(f'Time taken: {round(time.time() - start, 2)} seconds')\n",
    "\n",
    "        st.markdown('Finding similar bad Doc from testing data - %s rows'%dfTesting.shape[0])\n",
    "        start = time.time()\n",
    "        progressBar = st.progress(0)\n",
    "        #use trained model to find if there is report similar with any bad reports\n",
    "        similarList = []\n",
    "        indexList = []\n",
    "        scoreList = []\n",
    "        for no in range(dfTesting.shape[0]):\n",
    "            info = str(dfTesting['SUSPICION_DESC_CLEAN'].values[no])\n",
    "            similarBool, index, score = nlpSimilarity.findSimilarDocForBadDoc(info)\n",
    "            similarList.append(similarBool)\n",
    "            indexList.append(index)\n",
    "            scoreList.append(score)\n",
    "            percent = int(no*100/dfTesting.shape[0])\n",
    "            progressBar.progress(percent)\n",
    "        st.markdown(f'Time taken: {round((time.time() - start)/60, 2)} minutes')\n",
    "\n",
    "        #further data processing\n",
    "        with st.spinner(\"Final data processing\"):\n",
    "            start = time.time()\n",
    "            pandaDict = {'similarBool':similarList,\n",
    "                        'reportIndexList':indexList,\n",
    "                        'scoreList':scoreList}\n",
    "            dfResult = pd.DataFrame(pandaDict)\n",
    "            dfResult = pd.concat([dfResult, dfTesting.reset_index()], axis = 1)\n",
    "            dfResult.fillna('', inplace = True)\n",
    "\n",
    "            dfResult2 = mostSimilarReport(dfResult['reportIndexList'].values, dfResult['scoreList'].values)\n",
    "            colList = ['RECORD_ID', 'ANALYST', 'SUSPICION_DESC_CLEAN', 'TP', 'FN', 'FP', 'noFlag']\n",
    "            dfFinal = pd.concat([dfResult2, dfResult[colList].reset_index()], axis = 1).copy()\n",
    "\n",
    "            dfFinal['mostSimilarReportID'] = dfFinal['mostSimilarIndex'].map(lambda x:IndexToReportID_Dict[int(x)] \n",
    "                                                                             if str(x) not in ['', 'nan'] else '')\n",
    "\n",
    "            dfFinal['mostSimilarStrDesc'] = dfFinal['mostSimilarIndex'].map(lambda x:IndexToStrDesc_Dict[int(x)] \n",
    "                                                                             if str(x) not in ['', 'nan'] else '')\n",
    "        st.success(\"Final data processing is completed\")\n",
    "        st.markdown(f'Time taken: {round(time.time() - start, 2)} seconds')\n",
    "    \n",
    "    if file:\n",
    "        dfFinal = pd.read_csv(file)\n",
    "    \n",
    "    if dfFinal.shape[0]!=0:\n",
    "        csvFile = dfFinal.to_csv(index = False)\n",
    "        st.header(\"File Download - A Workaround for small data\")\n",
    "        b64 = base64.b64encode(csvFile.encode()).decode()  # some strings <-> bytes conversions necessary here\n",
    "        href = f'<a href=\"data:file/csv;base64,{b64}\">Download CSV File</a> (right-click and save as &lt;some_name&gt;.csv)'\n",
    "        st.markdown(href, unsafe_allow_html=True)\n",
    "\n",
    "        st.title('Evaluation')\n",
    "        #calculate how many false negative\n",
    "        successNum = sum(np.logical_and(pd.notnull(dfFinal['mostSimilarIndex']).values, dfFinal['FN'].values))\n",
    "        successPercent = round((successNum * 100/sum(dfFinal['FN'])),2)\n",
    "        st.markdown(\"Number of Reports is Flagged by Analyst (Rule does not flag):%s\"%sum(dfFinal['FN']))\n",
    "        st.markdown(\"Number of Reports is Flagged by Doc Similarity (Rule does not flag):%s\"%successNum)\n",
    "        st.markdown(\"Success Rate:%s percent\"%successPercent)\n",
    "        \n",
    "        st.cache()\n",
    "        st.title('Check results')\n",
    "        badReportList = list(map(int, dfFinal[(dfFinal['FN']) & \n",
    "                                              (pd.notnull(dfFinal['mostSimilarIndex']))]['mostSimilarIndex'].unique()))\n",
    "        badReportIDList = [IndexToReportID_Dict[i] for i in badReportList]\n",
    "        reportID = st.selectbox('Bad Report ID', badReportIDList)\n",
    "        reportInd = ReportIDToIndex_Dict[reportID]\n",
    "        #prepare series to display\n",
    "        series = pd.Series(IndexToStrDesc_Dict[reportInd])\n",
    "        series.name = 'Description'\n",
    "        table = pd.DataFrame().append(series)\n",
    "        table.columns = ['']\n",
    "        st.table(table)\n",
    "        \n",
    "        recordIDtoScoreDict = dict(sorted(dfFinal[dfFinal['mostSimilarIndex'].map(lambda x:x == float(reportInd))]\\\n",
    "                             .set_index('RECORD_ID')['mostSimilarScore']\\\n",
    "                             .to_dict().items(), key = operator.itemgetter(1), reverse = True))\n",
    "        \n",
    "        similarReportIDList = list(recordIDtoScoreDict.keys())\n",
    "        similarReportID = st.selectbox('Similar Report ID', similarReportIDList)\n",
    "        series = pd.Series(dfFinal[dfFinal['RECORD_ID'] == similarReportID]['SUSPICION_DESC'])\n",
    "        series.name = 'Description'\n",
    "        table = pd.DataFrame().append(series)\n",
    "        table.columns = ['']\n",
    "        st.write('Similarity Score: %s'%recordIDtoScoreDict[similarReportID])\n",
    "        st.table(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'117/SL/570603-X/S/2019/000652': '0.90683454',\n",
       " '117/SL/570603-X/S/2019/000524': '0.9054018',\n",
       " 'DA/002/S/2019/003980': '0.90137756',\n",
       " '117/SL/570603-X/S/2019/000639': '0.8999478',\n",
       " '117/SL/570603-X/S/2019/000552': '0.8995643',\n",
       " 'DA/002/S/2019/004603': '0.8989546',\n",
       " '117/SL/570603-X/S/2019/000605': '0.89864916',\n",
       " '117/SL/570603-X/S/2019/000574': '0.89546895',\n",
       " '117/SL/570603-X/S/2019/000740': '0.8940367',\n",
       " '117/SL/570603-X/S/2019/000584': '0.893069',\n",
       " '117/SL/570603-X/S/2019/000633': '0.8927595',\n",
       " '117/SL/570603-X/S/2019/000704': '0.8915929',\n",
       " '117/SL/570603-X/S/2019/000670': '0.89143133',\n",
       " '117/SL/570603-X/S/2019/000662': '0.889781',\n",
       " 'AP/018/S/2019/001000': '0.8897016',\n",
       " 'AP/018/S/2019/001015': '0.8896949',\n",
       " '117/SL/570603-X/S/2019/000734': '0.8885196',\n",
       " 'AP/018/S/2019/000975': '0.887955',\n",
       " '117/SL/570603-X/S/2019/000694': '0.88775545',\n",
       " '117/SL/570603-X/S/2019/000540': '0.8865546',\n",
       " 'AP/001/S/2019/000386': '0.88641894',\n",
       " '117/SL/570603-X/S/2019/000617': '0.88628036',\n",
       " 'AP/018/S/2019/001042': '0.88279897',\n",
       " 'AP/001/S/2019/000394': '0.88231605',\n",
       " '117/SL/570603-X/S/2019/000538': '0.88173795',\n",
       " '117/SL/0652/B/S/2019/000007': '0.88096744',\n",
       " '117/SL/570603-X/S/2019/000723': '0.8802123',\n",
       " 'DA/002/S/2019/004208': '0.87680423',\n",
       " '117/SL/570603-X/S/2019/000659': '0.8758151',\n",
       " '117/SL/570603-X/S/2019/000623': '0.8739144',\n",
       " 'DA/002/S/2019/004232': '0.87090796',\n",
       " '117/SL/570603-X/S/2019/000663': '0.87051576',\n",
       " '117/SL/570603-X/S/2019/000730': '0.87021184',\n",
       " 'AP/018/S/2019/000963': '0.870093',\n",
       " '117/SL/570603-X/S/2019/000518': '0.8693423',\n",
       " 'DA/002/S/2019/004453': '0.8690666',\n",
       " 'AC/003/S/2019/008271': '0.86873746',\n",
       " '117/SL/570603-X/S/2019/000667': '0.8660136',\n",
       " '117/SL/570603-X/S/2019/000702': '0.86566955',\n",
       " '117/SL/570603-X/S/2019/000583': '0.86514485',\n",
       " 'AC/011/S/2019/000040': '0.8651414',\n",
       " '117/SL/570603-X/S/2019/000645': '0.864862',\n",
       " 'EB/039/S/2019/000002F': '0.8648196',\n",
       " 'AP/001/S/2019/000343': '0.86418027',\n",
       " '117/SL/570603-X/S/2019/000547': '0.8626494',\n",
       " '117/SL/570603-X/S/2019/000593': '0.86143404',\n",
       " '117/SL/570603-X/S/2019/000731': '0.8609061',\n",
       " '117/SL/570603-X/S/2019/000532': '0.8608285',\n",
       " '117/SL/570603-X/S/2019/000714': '0.86059123',\n",
       " '117/SL/570603-X/S/2019/000708': '0.860491',\n",
       " '117/SL/570603-X/S/2019/000586': '0.8584458',\n",
       " 'AC/011/S/2019/000058': '0.8571935',\n",
       " 'AC/003/S/2019/009037': '0.85623795',\n",
       " 'AP/001/S/2019/000351': '0.8557184',\n",
       " 'AA/010/S/2019/023808': '0.8556605',\n",
       " 'AP/001/S/2019/000325': '0.85517406',\n",
       " '117/SL/570603-X/S/2019/000580': '0.8538453',\n",
       " 'AA/004/S/2019/005424': '0.85356927',\n",
       " '117/SL/570603-X/S/2019/000634': '0.8526524',\n",
       " 'AC/006/S/2019/004674': '0.85265064',\n",
       " '117/SL/570603-X/S/2019/000711': '0.85186505',\n",
       " 'AC/003/S/2019/007769': '0.8505876',\n",
       " 'AA/010/S/2019/021723': '0.8495573',\n",
       " 'AP/001/S/2019/000362': '0.8488121',\n",
       " 'DA/002/S/2019/004029': '0.8472537',\n",
       " 'AA/010/S/2019/020907': '0.8466069',\n",
       " 'AC/003/S/2019/008815': '0.84632045',\n",
       " 'AC/003/S/2019/007765': '0.84513015',\n",
       " 'DA/002/S/2019/004862': '0.84480655',\n",
       " '117/SL/570603-X/S/2019/000618': '0.844791',\n",
       " '117/SL/570603-X/S/2019/000716': '0.8445295',\n",
       " 'AC/003/S/2019/007790': '0.8442485',\n",
       " 'AA/010/S/2019/023699': '0.84369504',\n",
       " 'AC/003/S/2019/008327': '0.8434632',\n",
       " '117/SL/570603-X/S/2019/000546': '0.84224075',\n",
       " 'AA/010/S/2019/022298': '0.84095716',\n",
       " 'AA/010/S/2019/023810': '0.8405837',\n",
       " 'AA/010/S/2019/022075': '0.8392855',\n",
       " 'AC/003/S/2019/008090': '0.8376563',\n",
       " 'AA/010/S/2019/023395': '0.83609605',\n",
       " '117/SL/570603-X/S/2019/000561': '0.83605576',\n",
       " 'AA/010/S/2019/023105': '0.8356024',\n",
       " 'AF/003/S/2019/000866': '0.83347327',\n",
       " '117/SL/570603-X/S/2019/000596': '0.833401',\n",
       " 'AG/157/S/2019/000001F': '0.83313906',\n",
       " 'AA/020/S/2019/003459': '0.8325462',\n",
       " 'AP/001/S/2019/000342': '0.8311517',\n",
       " 'AF/001/S/2019/000342': '0.83112097',\n",
       " 'AC/003/S/2019/008827': '0.82991534',\n",
       " '117/SL/570603-X/S/2019/000733': '0.82983303',\n",
       " 'AA/010/S/2019/022417': '0.8295062',\n",
       " 'AA/010/S/2019/023671': '0.829401',\n",
       " '117/SL/570603-X/S/2019/000726': '0.8273639',\n",
       " 'AC/003/S/2019/008214': '0.8267873',\n",
       " 'AA/010/S/2019/022733': '0.8233866',\n",
       " 'AA/010/S/2019/021389': '0.82026064',\n",
       " '117/SL/570603-X/S/2019/000728': '0.8195',\n",
       " '117/SL/570603-X/S/2019/000587': '0.816068',\n",
       " '117/SL/570603-X/S/2019/000545': '0.81434447',\n",
       " 'DA/002/S/2019/003924': '0.8133859',\n",
       " 'AA/010/S/2019/022058': '0.8131222',\n",
       " 'AP/001/S/2019/000352': '0.81289935',\n",
       " 'AA/010/S/2019/022050': '0.8120774',\n",
       " 'AA/010/S/2019/023114': '0.8113229',\n",
       " 'AC/014/S/2019/001366': '0.81127256',\n",
       " '117/SL/570603-X/S/2019/000658': '0.81020236',\n",
       " 'AA/010/S/2019/023728': '0.81007713',\n",
       " 'AC/003/S/2019/007746': '0.8084892',\n",
       " 'DA/002/S/2019/004257': '0.8076',\n",
       " 'AP/001/S/2019/000389': '0.8067043',\n",
       " 'AC/003/S/2019/008809': '0.80579203',\n",
       " 'AA/010/S/2019/021809': '0.80444896',\n",
       " 'AA/010/S/2019/023724': '0.8041833',\n",
       " 'AC/003/S/2019/008971': '0.803936',\n",
       " '117/SL/570603-X/S/2019/000597': '0.802589',\n",
       " 'AC/003/S/2019/008802': '0.8021417',\n",
       " 'DA/002/S/2019/004264': '0.80186915',\n",
       " 'AC/003/S/2019/008156': '0.80184674',\n",
       " 'AC/003/S/2019/008696': '0.80146545',\n",
       " '117/WP/666325H/S/2019/000047': '0.80084246',\n",
       " 'AP/018/S/2019/001086': '0.8006959',\n",
       " 'AL/003/S/2019/000007': '0.8003174',\n",
       " 'AC/003/S/2019/008033': '0.80019516',\n",
       " 'AA/004/S/2019/006152': '0.80000526',\n",
       " '117/SL/570603-X/S/2019/000606': '0.79963875',\n",
       " 'AP/001/S/2019/000385': '0.7992507',\n",
       " 'DA/002/S/2019/004226': '0.7990465',\n",
       " 'AA/010/S/2019/021218': '0.7989642',\n",
       " 'DA/002/S/2019/003871': '0.7986551',\n",
       " 'AA/010/S/2019/023809': '0.79782474',\n",
       " 'AC/003/S/2019/008868': '0.7975004',\n",
       " 'AC/003/S/2019/008159': '0.797256',\n",
       " 'AC/003/S/2019/008824': '0.7968771',\n",
       " 'AA/010/S/2019/023859': '0.79666954',\n",
       " 'DA/002/S/2019/004331': '0.79528403',\n",
       " 'AC/003/S/2019/008044': '0.79400826',\n",
       " 'DA/002/S/2019/004015': '0.7933238',\n",
       " 'AF/003/S/2019/000877': '0.7929774',\n",
       " 'AA/010/S/2019/023685': '0.79252136',\n",
       " 'DA/002/S/2019/004063': '0.7921145',\n",
       " 'AC/003/S/2019/007768': '0.79205304',\n",
       " 'AA/010/S/2019/021761': '0.791372',\n",
       " '117/SL/570603-X/S/2019/000560': '0.79091007',\n",
       " 'AA/010/S/2019/024090': '0.79042727',\n",
       " 'AA/010/S/2019/021780': '0.79010063',\n",
       " 'AA/010/S/2019/020801': '0.7898037',\n",
       " 'DA/002/S/2019/004125': '0.7897647',\n",
       " 'DA/002/S/2019/004747': '0.788528',\n",
       " 'AA/010/S/2019/023113': '0.78797823',\n",
       " 'AC/003/S/2019/007685': '0.78775704',\n",
       " '117/SL/570603-X/S/2019/000520': '0.78758866',\n",
       " 'AA/010/S/2019/023730': '0.7874866',\n",
       " 'DA/002/S/2019/004030': '0.7839489',\n",
       " 'AA/010/S/2019/022424': '0.78392595',\n",
       " 'DA/002/S/2019/004071': '0.7836413',\n",
       " '117/SL/570603-X/S/2019/000713': '0.7834285',\n",
       " 'AA/010/S/2019/021759': '0.78171533',\n",
       " 'AC/014/S/2019/001585': '0.7812635',\n",
       " 'AP/001/S/2019/000368': '0.78088593',\n",
       " 'AC/003/S/2019/008298': '0.7808436',\n",
       " 'AA/010/S/2019/021635': '0.78075176',\n",
       " 'AA/004/S/2019/005335': '0.77921313',\n",
       " 'AC/003/S/2019/008803': '0.7787488',\n",
       " 'AA/010/S/2019/023118': '0.778692',\n",
       " 'DA/002/S/2019/004571': '0.77822214',\n",
       " 'DA/002/S/2019/004550': '0.77812797',\n",
       " 'AC/003/S/2019/007927': '0.777656',\n",
       " 'AA/010/S/2019/021731': '0.77664024',\n",
       " 'AC/003/S/2019/008756': '0.7745346',\n",
       " 'AA/010/S/2019/023665': '0.7744141',\n",
       " 'AF/003/S/2019/000902': '0.7742974',\n",
       " '117/SL/570603-X/S/2019/000678': '0.77166826',\n",
       " 'DA/002/S/2019/003848': '0.77045715',\n",
       " 'AC/003/S/2019/008190': '0.77006286',\n",
       " 'AA/010/S/2019/022317': '0.7695122',\n",
       " 'AA/010/S/2019/023721': '0.76673925',\n",
       " 'AA/010/S/2019/022657': '0.7649575',\n",
       " 'AC/014/S/2019/001348': '0.76495683',\n",
       " 'DA/002/S/2019/004399': '0.76443505',\n",
       " 'AC/003/S/2019/008203': '0.7641329',\n",
       " 'DA/002/S/2019/004616': '0.7632074',\n",
       " 'DA/002/S/2019/004473': '0.7605495',\n",
       " 'AP/011/S/2019/000688': '0.76039714',\n",
       " 'AA/020/S/2019/002973': '0.7599934',\n",
       " 'DA/002/S/2019/004359': '0.7593149',\n",
       " 'DA/002/S/2019/004698': '0.7590429',\n",
       " 'AA/010/S/2019/022065': '0.7585269',\n",
       " 'AP/011/S/2019/000787': '0.7578267',\n",
       " 'AC/003/S/2019/007730': '0.75751287',\n",
       " 'AP/011/S/2019/000708': '0.7565356',\n",
       " 'AC/003/S/2019/007789': '0.7562708',\n",
       " 'AA/010/S/2019/023668': '0.7560746',\n",
       " 'AA/010/S/2019/023669': '0.7551408',\n",
       " 'DA/002/S/2019/004251': '0.7549678',\n",
       " 'AA/010/S/2019/023394': '0.75490195',\n",
       " 'AA/010/S/2019/021725': '0.7547474',\n",
       " 'AA/010/S/2019/023045': '0.75365853',\n",
       " 'AC/003/S/2019/008636': '0.7531471',\n",
       " 'DA/002/S/2019/004410': '0.75302255',\n",
       " 'AF/003/S/2019/000888': '0.752962',\n",
       " 'AA/010/S/2019/021726': '0.7513387',\n",
       " 'AA/010/S/2019/022447': '0.7508979',\n",
       " 'DA/002/S/2019/004118': '0.7504726'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "#  sorted(x.items(), key=operator.itemgetter(1))\n",
    "recordIDtoScoreDict = sorted(dfFinal[dfFinal['mostSimilarIndex'].map(lambda x:x == '2')]\\\n",
    "                     .set_index('RECORD_ID')['mostSimilarScore']\\\n",
    "                     .to_dict().items(), key = operator.itemgetter(1), reverse = True)\n",
    "dict(recordIDtoScoreDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2', '', '417'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfFinal['mostSimilarIndex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(chart_html))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TestEnv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
