{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_f1_score\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "import pickle\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import itertools\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Features - CRF: \n",
    "<ul>\n",
    "<li>bias</li> \n",
    "<li>lower case of term</li>\n",
    "<li>if term is upper case</li>\n",
    "<li>if term is sentence case</li>\n",
    "<li>if term is digit</li>\n",
    "<li>postag</li>\n",
    "<li>last 2, 3 characters</li>\n",
    "<li>last 2 characters from pos</li>\n",
    "<li> all above features except bias for preceding term if current term is not beginning of sentence</li>\n",
    "<li> all above features except bias for following term if current term is not end of sentence</li>\n",
    "<li> only for beginning of sentence, BOS = True and ending of sentence, EOS = True</li>\n",
    "</ul>\n",
    "\n",
    "first term/end term in a sentence - 15 features\n",
    "other term - 19 features\n",
    "other term (only term in a sentence) - 11 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Features - CRF in our case: \n",
    "<ul>\n",
    "<li>bias</li> \n",
    "<li>lower case of term</li>\n",
    "<li>if term is upper case</li>\n",
    "<li>if term is sentence case</li>\n",
    "<li>if term is digit</li>\n",
    "<li>last 2, 3 characters</li>\n",
    "<li> all above features except bias for preceding term</li>\n",
    "<li> all above features except bias for following term</li>   \n",
    "<li>beginning of sentence?</li>\n",
    "<li>end of sentence?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import training data\n",
    "recordDictBack = []\n",
    "jsonFile = \"D:\\\\Users\\\\figohjs\\\\Documents\\\\NLP\\\\NER\\\\data\\\\training\\\\2020-12-04_LablledStr.json\"\n",
    "\n",
    "with open(jsonFile, 'r', encoding='utf-8') as input_file:\n",
    "    for row in input_file.readlines():\n",
    "        recordDictBack.append(json.loads(row))\n",
    "\n",
    "#all desc\n",
    "descList = [i['content'] for i in recordDictBack]\n",
    "\n",
    "#all tag\n",
    "tagList = [i['tagList'] for i in recordDictBack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(descList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json file from NER label app contains duplicated record coz it s saved time by time\n",
    "def cleanRecord(result): \n",
    "    finalList = []\n",
    "    \n",
    "    resultDict = [{i['content']:i['tagList'] for i in result}]\n",
    "    dictKeys = [key for i in resultDict for key,val in i.items()]\n",
    "    \n",
    "    for finalKey in set(dictKeys):\n",
    "        tempDict = {}\n",
    "        #take first entry of records with same content\n",
    "        tagVal = [val for i in resultDict for key, val in i.items() if key == finalKey][0]\n",
    "        contentVal = finalKey\n",
    "        tempDict['tagList'] = tagVal\n",
    "        tempDict['content'] = contentVal\n",
    "        finalList.append(tempDict)\n",
    "\n",
    "    return finalList\n",
    "\n",
    "#from yan ling - extra 200 rows\n",
    "yanLingJsonFolder = \"D:\\\\Users\\\\figohjs\\\\Documents\\\\NLP\\\\NER\\\\data\\\\training\\\\Labelled_200_20201110\"\n",
    "\n",
    "yanLingRecord = []\n",
    "for yanLingFile in os.listdir(yanLingJsonFolder):\n",
    "    with open(yanLingJsonFolder + '\\\\' + yanLingFile) as input_file:\n",
    "        for row in input_file.readlines():\n",
    "            yanLingRecord.append(json.loads(row))    \n",
    "\n",
    "yanLingFinalRecord = cleanRecord(yanLingRecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all desc\n",
    "descListYL = [i['content'] for i in yanLingFinalRecord]\n",
    "\n",
    "#all tag\n",
    "tagListYL = [i['tagList'] for i in yanLingFinalRecord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yanLingFinalRecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CUSTOMER (ABDUL MUTALIB MAULA ABDUL RAHIM, IC NO. 880704525453) OPENED A SAVINGS ACCOUNT (NO. 7071379494) WITH KHOO HUN YEANG STREET, KUCHING CIMB BRANCH ON 21 AUGUST 2018. BASED ON THE BANK'S RECORD, CUSTOMER IS A SELF-EMPLOYED HAWKER.\\r\\r\\n\\r\\r\\nREVIEW ON THE ACCOUNT (COVERING THE PERIOD BETWEEN 21 AUGUST 2018 AND 31 MARCH 2019), TOTAL DEPOSITS RM 85,236.04 (26 COUNTS) AND TOTAL WITHDRAWALS RM 84,882.50 (66 COUNTS) WERE MADE TO CUSTOMER'S ACCOUNT. NOTICED THAT MAJORITY OF THE TRANSACTIONS WERE MADE IN MARCH 2019. THE CUSTOMER RECEIVED FUNDS FROM MULTIPLE INDIVIDUALS WHERE THE PURPOSE OF TRANSACTIONS IS UNKNOWN. THE FUND IS FOLLOWED BY IMMEDIATE WITHDRAWAL OR INSTANT TRANSFERS TO THE FOLLOWING PARTIES:\\r\\r\\nSUE SWEE HOCK (ACCOUNT WITH ABMB)\\r\\r\\nCYH STAR ENTERPRISE (ACCOUNT WITH HONGLEONG BANK AND PUBLIC BANK)\\r\\r\\nPANG JUNG HS (ACCOUNT WITH PUBLIC BANK)\\r\\r\\nMUHAMMAD JEFRI B (ACCOUNT WITH BMMB)\\r\\r\\n\\r\\r\\nTHE BANK NOTED THAT THERE IS A POLICE REPORT (PADUNGAN/002472/19) LODGED ON THE CUSTOMER AND HAS BEEN REPORTED TO BE INVOLVED IN FRAUDULENT ACTIVITY. ENCLOSED IS A COPY OF THE POLICE REPORT. \\r\\r\\n\\r\\r\\nHENCE, STR IS RAISED AS CUSTOMER IS SUSPECTED INVOLVING IN MACAU SCAM FRAUD ACTIVITIES.\\r\\r\\n NA\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first layer of text cleaning\n",
    "def cleanText(text):\n",
    "    #special chars list\n",
    "    scList = ['\\si.e.\\s']\n",
    "    \n",
    "    #replace \\n with ' '\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    #remove null in end of sentence\n",
    "    text = re.sub('null $', '', text, flags = re.I)\n",
    "    #remove rm as training data does not have rm\n",
    "    text = re.sub('(rm|myr)\\s*(\\d+)', r'\\2', text, flags = re.I)\n",
    "    #remove special char \n",
    "    text = re.sub('|'.join(scList), '', text)\n",
    "    #remove additional spaces\n",
    "    text = re.sub('(\\s)+', r'\\1', text)        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processLabel(text, tag):\n",
    "    #store final tag result\n",
    "    tagResultList = []\n",
    "    \n",
    "    #tag: [[tag1, label1, color1]]\n",
    "    #tagDict = {tag1:label1}\n",
    "    tagDict = dict([(i[0], i[1]) for i in tag if i[1] in ['PERSON', 'ORG']])\n",
    "    nameList = [i[0] for i in tag if i[1] in ['PERSON', 'ORG']]\n",
    "    textList = text.split(' ')\n",
    "    labelDict = {\"PERSON\":\"per\", \"ORG\":\"org\"}\n",
    "    skipNo = []\n",
    "    for no, word in enumerate(textList):\n",
    "        NEfound = 0\n",
    "        if no not in skipNo:\n",
    "            cleanWord = re.sub(r'\\(|\\)|\\,|\\.', '', word).strip()\n",
    "            matchList = [i for i in nameList if cleanWord in i]\n",
    "            if len(matchList)!=0:\n",
    "                #find match in dict\n",
    "                tempDict = {i:j for i,j in tagDict.items() if re.search(cleanWord, i)}\n",
    "                for key,value in tempDict.items():\n",
    "                    wordLen = len(key.split(' '))\n",
    "                    words = re.sub(r'\\(|\\)|\\,|\\.', '', ' '.join(textList[no:no+wordLen])).strip()\n",
    "                    if words == key:\n",
    "                        NEfound = 1\n",
    "                    #add word index to skip no\n",
    "                        skipNo.append(no)\n",
    "                        tagResultList.append(\"B-\"+labelDict[tagDict[key]])\n",
    "                        for i in range(no+1, no+wordLen):\n",
    "                            skipNo.append(i)\n",
    "                            tagResultList.append(\"I-\"+labelDict[tagDict[key]])\n",
    "                        break\n",
    "                #if cannot find NE match\n",
    "                if NEfound == 0:\n",
    "                    tagResultList.append(\"O\")\n",
    "                            \n",
    "            else:\n",
    "                tagResultList.append(\"O\")\n",
    "    return tagResultList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagFinalResult = []\n",
    "tokenFinalResult = []\n",
    "badResult = []\n",
    "sentenceResult = []\n",
    "for no, desc in enumerate(descList):\n",
    "    cleanDesc = cleanText(desc)\n",
    "    result1 = processLabel(cleanDesc, tagList[no])\n",
    "    result2 = cleanDesc.split(' ')\n",
    "    tagFinalResult.append(result1)\n",
    "    tokenFinalResult.append(result2)\n",
    "    if len(result1)!=len(result2):\n",
    "        badResult.append(no)\n",
    "    sentenceResult.extend(repeat(\"Sentence\"+str(no+1), len(result2)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'I-per',\n",
       " 'I-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'I-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'I-org',\n",
       " 'I-org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagFinalResult[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten both list\n",
    "tagFinalResult = list(itertools.chain(*tagFinalResult))\n",
    "tokenFinalResult = list(itertools.chain(*tokenFinalResult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179124, 179124)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagFinalResult), len(tokenFinalResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179124, 3)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"Sentence\":sentenceResult,\n",
    "                   \"Token\":tokenFinalResult,\n",
    "                   \"Tag\":tagFinalResult})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = \"D:\\\\Users\\\\figohjs\\\\Documents\\\\NLP\\\\NER\\\\data\\\\training\\\\2021-01-14_InternalDataCRF.csv\"\n",
    "df.to_csv(csvFile, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use External Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default features used in nltk\n",
    "def word2features(sent, i):\n",
    "    word = str(sent[i][0])\n",
    "#     postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "#         'ori':word,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "#         'postag': postag,\n",
    "#         'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = str(sent[i-1][0])\n",
    "#         postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "#             '-1:postag': postag1,\n",
    "#             '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        #beginning of speech\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = str(sent[i+1][0])\n",
    "#         postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "#             '+1:postag': postag1,\n",
    "#             '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        #end of speech\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n",
    "\n",
    "def generateFeatures(descList):\n",
    "    featuresList = []\n",
    "    \n",
    "    for desc in descList: \n",
    "        sample = [(i,) for i in desc.split(' ')]\n",
    "        sampleFeatures = [sent2features(i) for i in [sample]]\n",
    "        featuresList.append(sampleFeatures[0])\n",
    "    \n",
    "    return featuresList\n",
    "\n",
    "# This is a class te get sentence. The each sentence will be list of tuples with its tag and pos.\n",
    "class sentence(object):\n",
    "    def __init__(self, df):\n",
    "        self.n_sent = 1\n",
    "        self.df = df\n",
    "        self.empty = False\n",
    "        agg = lambda s : [(w, p, t) for w, p, t in zip(s['Word'].values.tolist(),\n",
    "                                                       s['POS'].values.tolist(),\n",
    "                                                       s['Tag'].values.tolist())]\n",
    "        self.grouped = self.df.groupby(\"Sentence #\").apply(agg)\n",
    "        self.sentences = [s for s in self.grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated time: 0:08:00.540370\n"
     ]
    }
   ],
   "source": [
    "#Reading the csv file\n",
    "csvFile = \"D:/Users/figohjs/Documents/NLP/NER/data/raw/ner_dataset.csv\"\n",
    "df = pd.read_csv(csvFile, encoding = \"ISO-8859-1\")\n",
    "\n",
    "#filling column of sentence #\n",
    "df = df.fillna(method = 'ffill')\n",
    "\n",
    "#generate features\n",
    "# featureList = generateFeatures(descList)\n",
    "sentences = sentence(df)\n",
    "#for each sentence, a list of tuples: (token, pos, tag)\n",
    "# [('Iranian', 'JJ', 'B-gpe'),\n",
    "#  ('officials', 'NNS', 'O'),]\n",
    "allFullSentences = sentences.sentences\n",
    "\n",
    "#prepare X and Y\n",
    "#for each sentence, a list of dict: {features}\n",
    "X = [sent2features(s) for s in allFullSentences]\n",
    "y = [sent2labels(s) for s in allFullSentences]\n",
    "\n",
    "#remove pos features as testing dataset doesnt possess them\n",
    "X2 = []\n",
    "for i in X:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp.append({key:value for key,value in j.items() if 'postag' not in key})\n",
    "    X2.append(temp)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size = 0.3)\n",
    "\n",
    "#model training\n",
    "start = datetime.now()\n",
    "crf2 = CRF(algorithm = 'lbfgs',\n",
    "             c1 = 0.1,\n",
    "             c2 = 0.1,\n",
    "             max_iterations = 100,\n",
    "             all_possible_transitions = False)\n",
    "# crf2.fit(X_train, y_train)\n",
    "crf2.fit(X2, y)\n",
    "time = datetime.now() - start\n",
    "print(\"Estimated time: %s\"%time)\n",
    "\n",
    "#save model in pickle format\n",
    "filename = 'D:/Users/figohjs/Documents/NLP/NER/Model/2020-01-14_CRFmodel_Externalv1.sav'\n",
    "pickle.dump(crf2, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use internal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the csv file\n",
    "csvFile = \"D:\\\\Users\\\\figohjs\\\\Documents\\\\NLP\\\\NER\\\\data\\\\training\\\\2021-01-14_InternalDataCRF.csv\"\n",
    "dfInternal = pd.read_csv(csvFile, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default features used in nltk\n",
    "def word2features(sent, i):\n",
    "    word = str(sent[i][0])\n",
    "#     postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "#         'ori':word,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "#         'postag': postag,\n",
    "#         'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = str(sent[i-1][0])\n",
    "#         postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "#             '-1:postag': postag1,\n",
    "#             '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        #beginning of speech\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = str(sent[i+1][0])\n",
    "#         postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "#             '+1:postag': postag1,\n",
    "#             '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        #end of speech\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]\n",
    "\n",
    "def generateFeatures(descList):\n",
    "    featuresList = []\n",
    "    \n",
    "    for desc in descList: \n",
    "        sample = [(i,) for i in desc.split(' ')]\n",
    "        sampleFeatures = [sent2features(i) for i in [sample]]\n",
    "        featuresList.append(sampleFeatures[0])\n",
    "    \n",
    "    return featuresList\n",
    "\n",
    "# This is a class te get sentence. The each sentence will be list of tuples with its tag and pos.\n",
    "class sentence(object):\n",
    "    def __init__(self, df):\n",
    "        self.n_sent = 1\n",
    "        self.df = df\n",
    "        self.empty = False\n",
    "        agg = lambda s : [(w, t) for w, t in zip(s['Token'].values.tolist(),\n",
    "                                                s['Tag'].values.tolist())]\n",
    "        self.grouped = self.df.groupby(\"Sentence\").apply(agg)\n",
    "        self.sentences = [s for s in self.grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate features\n",
    "# featureList = generateFeatures(descList)\n",
    "sentences = sentence(dfInternal)\n",
    "#for each sentence, a list of tuples: (token, pos, tag)\n",
    "# [('Iranian', 'JJ', 'B-gpe'),\n",
    "#  ('officials', 'NNS', 'O'),]\n",
    "allFullSentences = sentences.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare X and Y\n",
    "#for each sentence, a list of dict: {features}\n",
    "X = [sent2features(s) for s in allFullSentences]\n",
    "y = [sent2labels(s) for s in allFullSentences]\n",
    "\n",
    "#remove pos features as testing dataset doesnt possess them\n",
    "X2 = []\n",
    "for i in X:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp.append({key:value for key,value in j.items() if 'postag' not in key})\n",
    "    X2.append(temp)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated time: 0:00:30.381431\n"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "start = datetime.now()\n",
    "crf = CRF(algorithm = 'lbfgs',\n",
    "             c1 = 0.1,\n",
    "             c2 = 0.1,\n",
    "             max_iterations = 100,\n",
    "             all_possible_transitions = False)\n",
    "crf.fit(X2, y)\n",
    "time = datetime.now() - start\n",
    "print(\"Estimated time: %s\"%time)\n",
    "\n",
    "#save model in pickle format\n",
    "filename = 'D:/Users/figohjs/Documents/NLP/NER/Model/2021-01-14_CRFmodel_Internalv1.sav'\n",
    "pickle.dump(crf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"NAME: INTERNATIONAL OIL DESIGN & CONSTRUCTION SDN BHD BUSINESS REGISTRATION NO: 829060W DATE OF INCORPORATION: 11/08/2008 ADDRESS: 1ST FLOOR WORK @CLEARWATER, CHANGKAT SEMANTAN, OFF JALAN SEMANTAN, 50490 DAMANSARA HEIGHTS, KUALA LUMPUR CONTACT NO: 0320959506, 0323822320 ACCOUNT NO: 8000298329 ACCOUNT OPENING DATE: 03/09/2010 ACCOUNT STATUS: ACTIVE HOME BRANCH: 1408 - KUALA LUMPUR MAIN LATEST BALANCE: 0.00 OTHER ACCOUNTS: 8007061034, 800000149040 (FCA USD), 800014315430 (FCA EUR) PCT1: M RAMANATHAN A/L S M MEYYAPPAN (NRIC NO: 600923106815) PCT2: A MAHESWARY A/P S ARJUNAN (NRIC NO: 660818106180) PCT3: MAZIAR MODARRES SADEGHI MAJID (PASSPORT NO: P95423858 - IRAN) PCT4: SEYEDABOLGHASEM SHEYKHOLESLAMI (PASSPORT NO: R26394391 - IRAN) PCT5: MAJID MALEK (PASSPORT NO: U27015144 - IRAN) INTERNATIONAL OIL DESIGN & CONSTRUCTION SDN BHD (IODC) INVOLVED IN OIL AND GAS INDUSTRIES AND PROVIDE SERVICES IN ENGINEERING, PROCUREMENT, CONSTRUCTION AND MANAGEMENT. IODC HAS BEEN BANKING WITH THE BANK SINCE 08/04/2009 WITH TWO FOREIGN CURRENCIES ACCOUNTS (EUR AND USD) AS WELL AS OPERATING ACCOUNT IN MYR. THE COMPANY IS MANAGED BY SOME LOCALS AND FEW IRANIANS. IODC HAS OPTED TO CLOSE ALL OF THEIR ACCOUNTS WITH THE BANK AND THEY HAVE GIVEN INSTRUCTION TO REMIT OUT THE BALANCES TO A THIRD PARTY ACCOUNT I.E LEOCO OIL LIMITED, ACCOUNT WITH MAYBANK KL. THE CONSOLIDATED FUNDS TRANSFERS WHICH TOOK PLACE ON 16/05/2019 AS FOLLOWS: EUR ACCOUNT - EUR4,480,181.86 USD ACCOUNT - USD82,316.02 MYR ACCOUNT - 997,115.26 THE FUNDS BEING TRANSFERRED TO MAYBANK'S ACCOUNT NUMBER 5142 3565 5872 (RM) AND 7142 3500 7146 (MULTI CURRENCY). STR IS RAISED ON INTERNATIONAL OIL DESIGN & CONSTRUCTION SDN BHD FOR THE LARGE TRANSFERS TO AN AGREED NOMINATED THIRD PARTY. BANK IS UNABLE TO ASCERTAIN THE RELATIONSHIP BETWEEN BOTH PARTIES. NA\""
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanDesc = cleanText(descListYL[1])\n",
    "cleanDesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stopword\n",
    "stopWordList = []\n",
    "txtFile = \"D:\\\\Users\\\\figohjs\\\\Documents\\\\NLP\\\\NER\\\\Data\\\\training\\\\stopwords.txt\"\n",
    "with open(txtFile, 'r') as myfile:\n",
    "    for row in myfile.readlines():\n",
    "        stopWordList.append(re.sub('\\n','',row))\n",
    "\n",
    "#import surname\n",
    "surnameList = []\n",
    "txtFile = \"D:\\\\Users\\\\figohjs\\\\Documents\\\\NLP\\\\NER\\\\Data\\\\training\\\\surname.txt\"\n",
    "with open(txtFile, 'r') as myfile:\n",
    "    for row in myfile.readlines():\n",
    "        surnameList.append(re.sub('\\n','',row))\n",
    "\n",
    "#tag dictionary\n",
    "tagDict = {'org':'ORG', 'per':'PERSON', 'geo': 'GEO'}\n",
    "\n",
    "def getNamedEntity(records, text):\n",
    "    finalResult = []\n",
    "    for noRow, row in enumerate(records):\n",
    "        temp = []\n",
    "        for noTerm, term in enumerate(row):\n",
    "            #if token is beginning of org or per\n",
    "            if term in ['B-' + i for i in tagDict.keys()]:\n",
    "                tagType = term.split('-')[1]\n",
    "                namedEnt = text[noRow][noTerm]\n",
    "                #if current term is not the last term of the row\n",
    "                if (noTerm + 1) != len(row):\n",
    "                    if row[noTerm + 1] != ('I-' + tagType):\n",
    "                        tempResult = checkTuple((namedEnt, tagDict[tagType]))\n",
    "                        if tempResult:\n",
    "                            temp.append(tempResult)\n",
    "                            \n",
    "                else:\n",
    "                    tempResult = checkTuple((namedEnt, tagDict[tagType]))\n",
    "                    if tempResult:\n",
    "                        temp.append(tempResult)\n",
    "\n",
    "            #if token is inside org or per\n",
    "            elif term in ['I-org', 'I-per', 'I-geo']:\n",
    "                tagType = term.split('-')[1]\n",
    "                namedEnt = ' '.join([namedEnt, text[noRow][noTerm]])\n",
    "                #if current term is not the last term of the row\n",
    "                if (noTerm + 1) != len(row):\n",
    "                    if row[noTerm + 1] != ('I-' + tagType):\n",
    "                        tempResult = checkTuple((namedEnt, tagDict[tagType]))\n",
    "                        if tempResult:\n",
    "                            temp.append(tempResult)   \n",
    "\n",
    "                else:\n",
    "                    tempResult = checkTuple((namedEnt, tagDict[tagType]))\n",
    "                    if tempResult:\n",
    "                        temp.append(tempResult)\n",
    "                        \n",
    "        finalResult.append(temp)\n",
    "            \n",
    "    return finalResult  \n",
    "\n",
    "def checkTuple(tupleResult):\n",
    "    if tupleResult[1] in ['PERSON', 'GEO']:\n",
    "        if re.search('berhad|bhd', tupleResult[0], flags = re.I):\n",
    "            return (tupleResult[0], 'ORG')\n",
    "        else:\n",
    "            #filter out geo\n",
    "            if tupleResult[1] == 'GEO':\n",
    "                return None\n",
    "            else:\n",
    "                return tupleResult\n",
    "            \n",
    "    elif tupleResult[1] == 'ORG':\n",
    "        #put chinese name back as label\n",
    "        if len(tupleResult[0].split(' ')) == 3 and not re.search('berhad|bhd', tupleResult[0], flags = re.I):\n",
    "            if tupleResult[0].split(' ')[0].lower() in surnameList:\n",
    "                return (tupleResult[0], 'PERSON')\n",
    "            else:\n",
    "                return tupleResult\n",
    "        else:\n",
    "            return tupleResult\n",
    "    else:\n",
    "        return tupleResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features engineering\n",
    "desc2 = [(i,) for i in cleanDesc.split(' ')]\n",
    "descFeatures = [sent2features(i) for i in [desc2]]\n",
    "y_pred = crf.predict(descFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"NAME: INTERNATIONAL OIL DESIGN & CONSTRUCTION SDN BHD BUSINESS REGISTRATION NO: 829060W DATE OF INCORPORATION: 11/08/2008 ADDRESS: 1ST FLOOR WORK @CLEARWATER, CHANGKAT SEMANTAN, OFF JALAN SEMANTAN, 50490 DAMANSARA HEIGHTS, KUALA LUMPUR CONTACT NO: 0320959506, 0323822320 ACCOUNT NO: 8000298329 ACCOUNT OPENING DATE: 03/09/2010 ACCOUNT STATUS: ACTIVE HOME BRANCH: 1408 - KUALA LUMPUR MAIN LATEST BALANCE: 0.00 OTHER ACCOUNTS: 8007061034, 800000149040 (FCA USD), 800014315430 (FCA EUR) PCT1: M RAMANATHAN A/L S M MEYYAPPAN (NRIC NO: 600923106815) PCT2: A MAHESWARY A/P S ARJUNAN (NRIC NO: 660818106180) PCT3: MAZIAR MODARRES SADEGHI MAJID (PASSPORT NO: P95423858 - IRAN) PCT4: SEYEDABOLGHASEM SHEYKHOLESLAMI (PASSPORT NO: R26394391 - IRAN) PCT5: MAJID MALEK (PASSPORT NO: U27015144 - IRAN) INTERNATIONAL OIL DESIGN & CONSTRUCTION SDN BHD (IODC) INVOLVED IN OIL AND GAS INDUSTRIES AND PROVIDE SERVICES IN ENGINEERING, PROCUREMENT, CONSTRUCTION AND MANAGEMENT. IODC HAS BEEN BANKING WITH THE BANK SINCE 08/04/2009 WITH TWO FOREIGN CURRENCIES ACCOUNTS (EUR AND USD) AS WELL AS OPERATING ACCOUNT IN MYR. THE COMPANY IS MANAGED BY SOME LOCALS AND FEW IRANIANS. IODC HAS OPTED TO CLOSE ALL OF THEIR ACCOUNTS WITH THE BANK AND THEY HAVE GIVEN INSTRUCTION TO REMIT OUT THE BALANCES TO A THIRD PARTY ACCOUNT I.E LEOCO OIL LIMITED, ACCOUNT WITH MAYBANK KL. THE CONSOLIDATED FUNDS TRANSFERS WHICH TOOK PLACE ON 16/05/2019 AS FOLLOWS: EUR ACCOUNT - EUR4,480,181.86 USD ACCOUNT - USD82,316.02 MYR ACCOUNT - 997,115.26 THE FUNDS BEING TRANSFERRED TO MAYBANK'S ACCOUNT NUMBER 5142 3565 5872 (RM) AND 7142 3500 7146 (MULTI CURRENCY). STR IS RAISED ON INTERNATIONAL OIL DESIGN & CONSTRUCTION SDN BHD FOR THE LARGE TRANSFERS TO AN AGREED NOMINATED THIRD PARTY. BANK IS UNABLE TO ASCERTAIN THE RELATIONSHIP BETWEEN BOTH PARTIES. NA\""
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanDesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' INTERNATIONAL OIL DESIGN & CONSTRUCTION SDN BHD', 'ORG', '#8ef'],\n",
       " [' INTERNATIONAL OIL DESIGN & CONSTRUCTION SDN BHD', 'ORG', '#8ef'],\n",
       " ['M RAMANATHAN A/L S M MEYYAPPAN', 'PERSON', '#faa'],\n",
       " [' A MAHESWARY A/P S ARJUNAN', 'PERSON', '#faa'],\n",
       " ['MAZIAR MODARRES SADEGHI MAJID', 'PERSON', '#faa'],\n",
       " ['SEYEDABOLGHASEM SHEYKHOLESLAMI', 'PERSON', '#faa'],\n",
       " ['MAJID MALEK', 'PERSON', '#faa'],\n",
       " ['DAMANSARA HEIGHTS', 'LOC', '#fea'],\n",
       " [' KUALA LUMPUR', 'LOC', '#fea'],\n",
       " [' KUALA LUMPUR', 'LOC', '#fea'],\n",
       " ['IRAN', 'LOC', '#fea'],\n",
       " ['IRAN', 'LOC', '#fea'],\n",
       " ['IRAN', 'LOC', '#fea'],\n",
       " ['IRAN', 'LOC', '#fea'],\n",
       " ['MAYBANK ', 'BANK', '#afa'],\n",
       " ['829060W', 'ID', '#aaf'],\n",
       " ['600923106815', 'ID', '#aaf'],\n",
       " ['U27015144', 'ID', '#aaf'],\n",
       " ['P95423858', 'ID', '#aaf'],\n",
       " ['8000298329', 'ACCOUNT', '#ddd'],\n",
       " ['8007061034', 'ACCOUNT', '#ddd'],\n",
       " [' 800000149040', 'ACCOUNT', '#ddd'],\n",
       " ['800014315430', 'ACCOUNT', '#ddd'],\n",
       " ['660818106180', 'ACCOUNT', '#ddd'],\n",
       " ['660818106180', 'ACCOUNT', '#ddd']]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagListYL[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('M RAMANATHAN A/L S M MEYYAPPAN', 'PERSON'),\n",
       "  ('MAZIAR MODARRES SADEGHI MAJID', 'PERSON'),\n",
       "  ('SEYEDABOLGHASEM SHEYKHOLESLAMI', 'PERSON'),\n",
       "  ('MAJID MALEK', 'PERSON')]]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predList = getNamedEntity(y_pred, [cleanDesc.split(' ')])\n",
    "predList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TestEnv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
