{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy, re\n",
    "from spacy.attrs import ENT_IOB\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy import displacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.symbols import ORTH,LEMMA,POS\n",
    "from pathlib import Path\n",
    "from spacy.tokens import Span\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from spacy.strings import StringStore\n",
    "import json\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import training data\n",
    "recordDictBack = []\n",
    "jsonFile = \"D:\\\\Users\\\\figohjs\\\\Documents\\\\NLP\\\\NER\\\\data\\\\training\\\\2020-12-04_LablledStr.json\"\n",
    "\n",
    "with open(jsonFile, 'r', encoding='utf-8') as input_file:\n",
    "    for row in input_file.readlines():\n",
    "        recordDictBack.append(json.loads(row))\n",
    "\n",
    "#all desc\n",
    "descList = [i['content'] for i in recordDictBack]\n",
    "\n",
    "#all tag\n",
    "tagList = [i['tagList'] for i in recordDictBack]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### munirah's processing pipeline and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_case_gen_pattern(parsed_doc):\n",
    "    strg_2=''\n",
    "    passport=False\n",
    "    \n",
    "    #   Change format of capitalized words into proper case (specifically nouns: NNP, NNS, NNPS) \n",
    "    for token in parsed_doc:\n",
    "        tagged_sent = [(token.text, token.tag_)]\n",
    "        \n",
    "        normalized_sent = [w.capitalize() if (t in [\"NNP\",\"NNS\",\"NNPS\"] and w.isupper()) else w for (w,t) in tagged_sent]\n",
    "        strg = re.sub(\" (?=[\\.,'!?:;])\", \"\", ' '.join(normalized_sent))   \n",
    "\n",
    "        lowerList = ('name','cash','bank','inter','amlatfa','cdd','jalan','ic','cust','business','director', 'sole', 'proprietor','str', 'cdm','saving', 'rm', 'myr','place','branch')\n",
    "\n",
    "        if any(substring in strg.lower() for substring in lowerList):\n",
    "               strg=strg.lower()\n",
    "                \n",
    "        # convert the remaining uppercase to lowercase\n",
    "        if strg.isupper():\n",
    "            strg = strg.lower()\n",
    "        \n",
    "        # remove unnecessary punctuations \n",
    "        strg_2 += re.sub(r'[=*()]',r'',strg.strip())\n",
    "        strg_2 += ' '\n",
    "        \n",
    "            \n",
    "\n",
    "    # remove space before certain symbols like <space><.> or <space><,>\n",
    "    strg_2=re.sub(r'\\s+([?.!,:\"/\\'])', r'\\1', strg_2)\n",
    "    strg_2=strg_2.replace(\"Õ\",\" \").replace(\"ð\",\" \").replace(\"õ\",\" \")\n",
    "    strg_2=strg_2.replace(\"  \",\". \")\n",
    "     # remove space after certain symbols like <space><.> or <space><,>\n",
    "    strg_2=re.sub(r'([?/])+\\s', r'\\1', strg_2)\n",
    "    strg_2=strg_2.replace('\"','\\\\\"')\n",
    "    \n",
    "    if(strg_2.lower().__contains__('passport')):\n",
    "        passport=True\n",
    "    phrase_matcher(strg_2, passport)\n",
    "    \n",
    "#     print ('\\n')\n",
    "    \n",
    "    return strg_2.strip()\n",
    "\n",
    "\n",
    "# phrase matcher: shape\n",
    "def phrase_matcher(strg, p_e):\n",
    "    nlp = English()\n",
    "    \n",
    "    matcher = PhraseMatcher(nlp.vocab, attr=\"SHAPE\")\n",
    "    matcher.add(u\"STR_ID\", None, nlp(u\"aa/025/s/2016/000019\"), nlp(u\"aa/025/s/2016/000075\"))\n",
    "    matcher.add(u\"PERSON_ID\", None, nlp(u\"881102-08-5192\"))\n",
    "#     matcher.add(u\"PERSON_ID_2\", None, nlp(u\"661124085949\"), nlp(u\"710905125067\"))\n",
    "    matcher.add(u\"PASSPORT\", None, nlp(u\"r711493\"), nlp(u\"ma438972\"), nlp(u\"a3894268\"))\n",
    "    matcher.add(u\"PASSPORT_OR_NRIC\", None, nlp(u\"706251339\"), nlp(u\"720201106027\"), nlp(u\"nric: 811018105265\"), nlp(u\"710905125067\"))\n",
    "    stringstore = StringStore([u\"STR_ID\",u\"PERSON_ID\",u\"PASSPORT\",u\"PASSPORT_OR_NRIC\"])\n",
    "    \n",
    "#     doc = nlp(strg.lower())\n",
    "#     for match_id, start, end in matcher(doc):\n",
    "#         span = doc[start:end]\n",
    "#     #         with open(\"D:\\\\Users\\\\mcazwan\\\\Desktop\\\\fisitti\\\\str-id-patterns.jsonl\", \"a\") as text_file:\n",
    "#     #             text_file.write('{\\n\"label\": \"STR_ID\", [{\"TEXT\":' + span.text + '}]),\\n')\n",
    "        \n",
    "#         #  only allow 12 characters for person_id\n",
    "#         if match_id == stringstore[u\"PERSON_ID\"] or match_id == stringstore[u\"PERSON_ID_2\"]:\n",
    "#             if(len(span.text.replace(' ','').replace('-','')) == 12):\n",
    "#                 print(\"(person_id):\", doc[start:end])\n",
    "#         elif match_id == stringstore[u\"PASSPORT\"] and p_e == True:\n",
    "#         # check pattern\n",
    "#             if(7 <= len(span.text.replace(' ','').replace('-','')) <= 9):\n",
    "#                 print(\"(passport):\", doc[start:end])\n",
    "#         elif match_id == stringstore[u\"PASSPORT_OR_NRIC\"]:\n",
    "#             if(7 <= len(span.text.replace(' ','').replace('-','')) <= 9) and p_e == True:\n",
    "#                 print(\"(passport-digit):\", doc[start:end])\n",
    "                \n",
    "#             elif(len(span.text.replace(' ','').replace('-','')) == 12):\n",
    "#                 ic=span.text.replace(' ','').replace('-','')\n",
    "#                 if(int(ic[2:4]) <= 12 and int(ic[4:6]) <= 31 and int(ic[6:8]) <= 14):\n",
    "#                     print(\"(nric-digit):\", doc[start:end])\n",
    "#         elif match_id == stringstore[u\"STR_ID\"]:\n",
    "#             print(\"(str_id):\", doc[start:end])\n",
    "             \n",
    "\n",
    "# fix spaces\n",
    "def fix_space_tags(doc):\n",
    "    ent_iobs = doc.to_array([ENT_IOB])\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.is_space:\n",
    "            # Sets 'O' tag (0 is None, so I is 1, O is 2)\n",
    "            ent_iobs[i] = 2\n",
    "    \n",
    "    doc.from_array([ENT_IOB], ent_iobs.reshape((len(doc), 1)))\n",
    "    return doc\n",
    "\n",
    "def prevent_sentence_boundaries(doc):\n",
    "    for token in doc:\n",
    "        if not can_be_sentence_start(token):\n",
    "            token.is_sent_start = False\n",
    "    return doc\n",
    "\n",
    "\n",
    "def can_be_sentence_start(token):\n",
    "#     print(token.text,':',token.i,'\\n')\n",
    "#     create separator\n",
    "    \n",
    "    if token.i == 0:\n",
    "        return True\n",
    "    # We're not checking for is_title here to ignore arbitrary titlecased\n",
    "    # tokens within sentences\n",
    "    \n",
    "    elif token.nbor(-1).is_punct and token.nbor(-1).text not in [':','/','-']:\n",
    "        return True\n",
    "\n",
    "    elif token.nbor(-1).is_space:\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        return False\n",
    "\n",
    "spacyModel = 'en_core_web_md'\n",
    "nlp = spacy.load(spacyModel, disable=['ner'])\n",
    "nlp.add_pipe(prevent_sentence_boundaries, before=\"parser\")\n",
    "\n",
    "#     for other processing   \n",
    "munirahModel = 'D:\\\\Users\\\\figohjs\\\\Documents\\\\NLP\\\\Spacy\\\\NER\\\\trained-model\\\\NER_All_Labels_lg'\n",
    "nlp2 = spacy.load(munirahModel)\n",
    "nlp2.add_pipe(fix_space_tags, name=\"fix-ner\", before=\"ner\")\n",
    "nlp2.add_pipe(prevent_sentence_boundaries, before=\"parser\")\n",
    "\n",
    "jsonlFile = \"D:\\\\Users\\\\figohjs\\\\Documents\\\\NLP\\\\NER\\\\Notebook\\\\patterns.jsonl\"\n",
    "new_ruler = EntityRuler(nlp).from_disk(jsonlFile)\n",
    "nlp2.add_pipe(new_ruler, after='ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CUSTOMER (ABDUL MUTALIB MAULA ABDUL RAHIM, IC NO. 880704525453) OPENED A SAVINGS ACCOUNT (NO. 7071379494) WITH KHOO HUN YEANG STREET, KUCHING CIMB BRANCH ON 21 AUGUST 2018. BASED ON THE BANK'S RECORD, CUSTOMER IS A SELF-EMPLOYED HAWKER.\\r\\r\\n\\r\\r\\nREVIEW ON THE ACCOUNT (COVERING THE PERIOD BETWEEN 21 AUGUST 2018 AND 31 MARCH 2019), TOTAL DEPOSITS RM 85,236.04 (26 COUNTS) AND TOTAL WITHDRAWALS RM 84,882.50 (66 COUNTS) WERE MADE TO CUSTOMER'S ACCOUNT. NOTICED THAT MAJORITY OF THE TRANSACTIONS WERE MADE IN MARCH 2019. THE CUSTOMER RECEIVED FUNDS FROM MULTIPLE INDIVIDUALS WHERE THE PURPOSE OF TRANSACTIONS IS UNKNOWN. THE FUND IS FOLLOWED BY IMMEDIATE WITHDRAWAL OR INSTANT TRANSFERS TO THE FOLLOWING PARTIES:\\r\\r\\nSUE SWEE HOCK (ACCOUNT WITH ABMB)\\r\\r\\nCYH STAR ENTERPRISE (ACCOUNT WITH HONGLEONG BANK AND PUBLIC BANK)\\r\\r\\nPANG JUNG HS (ACCOUNT WITH PUBLIC BANK)\\r\\r\\nMUHAMMAD JEFRI B (ACCOUNT WITH BMMB)\\r\\r\\n\\r\\r\\nTHE BANK NOTED THAT THERE IS A POLICE REPORT (PADUNGAN/002472/19) LODGED ON THE CUSTOMER AND HAS BEEN REPORTED TO BE INVOLVED IN FRAUDULENT ACTIVITY. ENCLOSED IS A COPY OF THE POLICE REPORT. \\r\\r\\n\\r\\r\\nHENCE, STR IS RAISED AS CUSTOMER IS SUSPECTED INVOLVING IN MACAU SCAM FRAUD ACTIVITIES.\\r\\r\\n NA\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "doc = nlp.pipe(iter(descList[:10]), batch_size=10, n_threads=4000) \n",
    "\n",
    "result = []\n",
    "for parsed_doc in doc:\n",
    "    #  add pipe\n",
    "#     doc_2 = nlp2(change_case(parsed_doc))\n",
    "    doc_2 = nlp2(change_case_gen_pattern(parsed_doc))\n",
    "    result.append(doc_2)\n",
    "\n",
    "predResultMunirah = []\n",
    "for record in result:\n",
    "    tempList = []\n",
    "    for entity in record.ents:\n",
    "        if entity.label_ in ['PERSON', 'ORG']:\n",
    "            tempList.append((entity, entity.label_))\n",
    "    predResultMunirah.append(tempList)\n",
    "\n",
    "#convert result and tag into str for saving purpose\n",
    "predResultMunirah2 = [[(str(j[0]),str(j[1])) for j in i] for i in predResultMunirah]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Abdul Mutalib Maula Abdul Rahim', 'PERSON'),\n",
       " ('Khoo Hun Yeang', 'PERSON'),\n",
       " ('Sue Swee Hock', 'PERSON'),\n",
       " ('Star enterprise', 'ORG'),\n",
       " ('Pang Jung Hs', 'PERSON'),\n",
       " ('Muhammad Jefri', 'PERSON'),\n",
       " ('Bmmb', 'ORG')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predResultMunirah2[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TestEnv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
